{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sopyjj/lumin8/blob/main/D1C1VC_Bootcamp_Neural_Network_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBrou3thi5L5"
      },
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "Harvard AI Bootcamp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a copy of this notebook! Editing directly will not be saved."
      ],
      "metadata": {
        "id": "rdR8-cp-9aGh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNmznXfi5L7"
      },
      "source": [
        "This notebook introduces you to PyTorch, an open source ML framework that allows you to build neural networks. PyTorch is the most widely used framework in research and has a huge developer community. Alternatives include [TensorFlow](https://www.tensorflow.org/), [JAX](https://github.com/google/jax#quickstart-colab-in-the-cloud) and [Caffe](http://caffe.berkeleyvision.org/).\n",
        "\n",
        "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format\n",
        "\n",
        "\n",
        "1.   PyTorch Basics\n",
        "2.   Models and Activation Functions\n",
        "3.   Gradients\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ad63Cg1CX1e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud5fbk0li5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e90614c-3ae7-4816-a8e0-84b9265be501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-ef889392170e>:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgba\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AupeGw3i5L9"
      },
      "source": [
        "## The Basics of PyTorch\n",
        "\n",
        "Let's start with importing PyTorch. The package is called `torch`, based on its original framework [Torch](http://torch.ch/). As a first step, we can check its version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIKiOeW8i5L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0009f35-20eb-4864-af7d-86d3c09064bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TlnR0QSi5L9"
      },
      "source": [
        "As in every machine learning framework, PyTorch provides functions that are stochastic like generating random numbers. However, a very good practice is to setup your code to be reproducible with the exact same random numbers. This is why we set a seed below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNpIaMn9i5L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa30be7c-e5a1-47cc-b48e-59715ea71530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d10a683e870>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(42) # Setting the seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBDxekHCi5L9"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration (more on that later).\n",
        "\n",
        "The name \"tensor\" is a generalization of familiar concepts. A vector is a 1-D tensor; a matrix a 2-D tensor.\n",
        "\n",
        "Most common functions from numpy can be used on tensors. Since numpy arrays are so similar to tensors, we can convert most tensors to numpy arrays (and back).\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "Let's first start by looking at different ways of creating a tensor. There are many possible options, the simplest one is to call `torch.Tensor` passing the desired shape as input argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V8K5IfYi5L-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803d5cc3-3bd0-4f69-ccea-3e196600734c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-3.5841e+35,  4.4864e-41, -3.5841e+35,  4.4864e-41],\n",
            "         [ 0.0000e+00,  4.4864e-41,  1.8788e+31,  1.7220e+22],\n",
            "         [ 2.1715e-18,  1.6707e-07,  6.8255e-07,  2.0801e+23]],\n",
            "\n",
            "        [[ 1.3722e-05,  1.3458e+22,  1.0373e-08,  5.4881e-05],\n",
            "         [ 5.2669e-08,  2.5811e-09,  1.4580e-19,  1.1495e+24],\n",
            "         [ 3.0881e+29,  1.5766e-19,  4.6304e+27,  7.2707e+31]]])\n"
          ]
        }
      ],
      "source": [
        "# Create a 3-dimensional tensor with dimensions 2x3x4\n",
        "# torch.Tensor takes as input the dimensions of the desired tensor, and outputs an allocated tensor object. torch.Tensor(2,2) creates a 2-D square tensor.\n",
        "# Print the tensor\n",
        "\n",
        "# TODO\n",
        "x =\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANxnx75i5L-"
      },
      "source": [
        "`torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory (hence the random numbers printed). To directly assign values to the tensor during initialization, there are many alternatives including:\n",
        "\n",
        "* `torch.zeros`: Creates a tensor filled with zeros\n",
        "* `torch.ones`: Creates a tensor filled with ones\n",
        "* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list elements you provide"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create a zero-valued tensor with 6 by 6 dimensions\n",
        "x =\n",
        "print(x)"
      ],
      "metadata": {
        "id": "FUI-HEb2ykmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edDPqfJai5L-"
      },
      "outputs": [],
      "source": [
        "# Create a tensor equivalent to\n",
        "# [[1, 2],\n",
        "#  [3, 4]]\n",
        "# TODO\n",
        "x =\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q22PbEl8i5L-"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]\n",
        "x =\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XKGgpGdmddYR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5zYQQeKi5ME"
      },
      "source": [
        "### GPU support\n",
        "\n",
        "A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. When comparing GPUs to CPUs, we can list the following main differences (credit: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/))\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/comparison_CPU_GPU.png?raw=1\" width=\"700px\"></center>\n",
        "\n",
        "GPUs can accelerate the training of your network up to a factor of $100$ which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn)). First, let's check whether you have a GPU available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_avail = torch.cuda.is_available()\n",
        "print(f\"Is the GPU available? {gpu_avail}\")"
      ],
      "metadata": {
        "id": "-fizvVRk6RkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ],
      "metadata": {
        "id": "uVFkMFJN6S-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a tensor and push it to the device:"
      ],
      "metadata": {
        "id": "cMWn67HZ6YeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(2, 3)\n",
        "x = x.to(device)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "id": "YlJ345pl6bJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5000, 5000)\n",
        "\n",
        "## CPU version\n",
        "start_time = time.time()\n",
        "_ = torch.matmul(x, x)\n",
        "end_time = time.time()\n",
        "print(f\"CPU time: {(end_time - start_time):6.5f}s\")\n",
        "\n",
        "## GPU version\n",
        "x = x.to(device)\n",
        "_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n",
        "# CUDA is asynchronous, so we need to use different timing functions\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "_ = torch.matmul(x, x)\n",
        "end.record()\n",
        "torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
        "print(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds"
      ],
      "metadata": {
        "id": "wQQ9UrEG6cQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the size of the operation and the CPU/GPU in your system, the speedup of this operation can be >50x."
      ],
      "metadata": {
        "id": "u3TOqA3R6gtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU operations have a separate seed we also want to set\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
        "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "nE22bWV76iAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUkpNWhVi5MG"
      },
      "source": [
        "## Example: Continuous XOR\n",
        "\n",
        "\n",
        "Given two binary inputs $x_1$ and $x_2$, the label to predict is $1$ if either $x_1$ or $x_2$ is $1$ while the other is $0$, or the label is $0$ in all other cases.\n",
        "\n",
        "The example became famous by the fact that a single neuron, i.e. a linear classifier, cannot learn this simple function.\n",
        "\n",
        "Hence, we will learn how to build a small neural network that can learn this function.\n",
        "\n",
        "To make it a little bit more interesting, we move the XOR into continuous space and introduce some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows:\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/continuous_xor.svg?raw=1\" width=\"350px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbORrz1-i5MG"
      },
      "source": [
        "### The model\n",
        "\n",
        "The package `torch.nn` defines a series of useful classes like linear networks layers, activation functions, loss functions etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1g9S-Ii5MG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfqwpskbi5MG"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mal34jiPi5MG"
      },
      "source": [
        "#### nn.Module\n",
        "\n",
        "In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class MyModule(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: Some init for my module\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: perform the calculation of the module.\n",
        "        pass\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "5ufrjBudzT7R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC3DpTili5MG"
      },
      "source": [
        "The forward function is where the computation of the module is taken place, and is executed when you call the module (`nn = MyModule(); nn(x)`).\n",
        "\n",
        "#### Simple classifier\n",
        "We can now make use of the pre-defined modules in the `torch.nn` package, and define our own small neural network. We will implement a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer according to the following graph.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/small_neural_network.svg?raw=1\" width=\"300px\"></center>\n",
        "\n",
        "The input neurons are shown in blue, which represent the coordinates $x_1$ and $x_2$ of a data point. The hidden neurons including a tanh activation are shown in white, and the output neuron in red.\n",
        "In PyTorch, we can define this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh6GDFzAi5MG"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        # TODO\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i0hgPZpi5MG"
      },
      "source": [
        "For the examples in this notebook, we will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJY37-kGi5MG"
      },
      "outputs": [],
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4Bs40cki5MG"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}, shape {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLmsc0ri5MG"
      },
      "source": [
        "### The data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeo4TGiyi5MG"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LipCn5zsi5MH"
      },
      "source": [
        "#### The dataset class\n",
        "\n",
        "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8MjhP5Ki5MH"
      },
      "outputs": [],
      "source": [
        "class XORDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, size, std=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.std = std\n",
        "        self.generate_continuous_xor()\n",
        "\n",
        "    def generate_continuous_xor(self):\n",
        "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
        "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
        "        # If x=y, the label is 0.\n",
        "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
        "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
        "        data += self.std * torch.randn(data.shape)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F70vAs3Wi5MH"
      },
      "outputs": [],
      "source": [
        "dataset = XORDataset(size=200)\n",
        "print(\"Size of dataset:\", len(dataset))\n",
        "print(\"Data point 0:\", dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md4tY5Fji5MH"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTP_xKaki5MH"
      },
      "outputs": [],
      "source": [
        "visualize_samples(dataset.data, dataset.label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjqE85dyi5MH"
      },
      "source": [
        "#### The data loader class\n",
        "\n",
        "The data loader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
        "\n",
        "\n",
        "* `batch_size`: Number of samples to stack per batch\n",
        "* `shuffle`: If True, the data is returned in a random order. This is important during training for introducing stochasticity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8bqDBZii5MH"
      },
      "outputs": [],
      "source": [
        "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGY-5MAui5MH"
      },
      "outputs": [],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs, data_labels = next(iter(data_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
        "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXG6AA42i5MH"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
        "\n",
        "1. Get a batch from the data loader\n",
        "2. Obtain the predictions from the model for the batch\n",
        "3. Calculate the loss based on the difference between predictions and labels\n",
        "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
        "5. Update the parameters of the model in the direction of the gradients\n",
        "\n",
        "We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odSZCvmCi5MH"
      },
      "source": [
        "#### Loss modules\n",
        "\n",
        "We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:\n",
        "\n",
        "$$\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]$$\n",
        "\n",
        "where $y$ are our labels, and $x$ our predictions, both in the range of $[0,1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2NrTVFvi5MH"
      },
      "outputs": [],
      "source": [
        "loss_module = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg2LyvsBi5MH"
      },
      "source": [
        "#### Stochastic Gradient Descent\n",
        "\n",
        "Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YvRdOqbi5MH"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbcMLzWKi5MH"
      },
      "source": [
        "The optimizer provides two useful functions: `optimizer.step()`, and `optimizer.zero_grad()`.\n",
        "\n",
        "The step function updates the parameters based on the gradients as explained above. The function `optimizer.zero_grad()` sets the gradients of all parameters to zero. If we call the `backward` function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them.\n",
        "\n",
        "This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call `optimizer.zero_grad()` before calculating the gradients of a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIPO_haxi5MH"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snj1cu_Pi5MH"
      },
      "outputs": [],
      "source": [
        "train_dataset = XORDataset(size=2500)\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjT-LWAOi5MI"
      },
      "outputs": [],
      "source": [
        "# Push model to device. Has to be only done once\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg4aby2ni5MI"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr6s777Ii5MI"
      },
      "outputs": [],
      "source": [
        "train_model(model, optimizer, train_data_loader, loss_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40noyFCi5MI"
      },
      "source": [
        "#### Saving a model\n",
        "\n",
        "After finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called `state_dict` from the model which contains all learnable parameters. For our simple model, the state dict contains the following entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl9K6qdwi5MI"
      },
      "outputs": [],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN17GAYqi5MI"
      },
      "outputs": [],
      "source": [
        "# torch.save(object, filename). For the filename, any extension can be used\n",
        "torch.save(state_dict, \"our_model.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyEttWdpi5MI"
      },
      "outputs": [],
      "source": [
        "# Load state dict from the disk (make sure it is the same name as above)\n",
        "state_dict = torch.load(\"our_model.tar\")\n",
        "\n",
        "# Create a new model and load the state\n",
        "new_model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "new_model.load_state_dict(state_dict)\n",
        "\n",
        "# Verify that the parameters are the same\n",
        "print(\"Original model\\n\", model.state_dict())\n",
        "print(\"\\nLoaded model\\n\", new_model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp_dMw_hi5MI"
      },
      "source": [
        "A detailed tutorial on saving and loading models in PyTorch can be found [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFrKYyBzi5MI"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo7i08E1i5MI"
      },
      "outputs": [],
      "source": [
        "test_dataset = XORDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp6McGdOi5MI"
      },
      "source": [
        "As metric, we will use accuracy which is calculated as follows:\n",
        "\n",
        "$$acc = \\frac{\\#\\text{correct predictions}}{\\#\\text{all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "where TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Cu4L2Vi5MI"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            # Determine prediction of model on dev set\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvpxXPN9i5MI"
      },
      "outputs": [],
      "source": [
        "eval_model(model, test_data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2YMWJA5i5MI"
      },
      "source": [
        "If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don't get such high scores on test sets of more complex tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm6xUtdLi5MI"
      },
      "source": [
        "#### Visualizing classification boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSe0XCZji5MI"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n",
        "def visualize_classification(model, data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Let's make use of a lot of operations we have learned above\n",
        "    model.to(device)\n",
        "    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n",
        "    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n",
        "    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n",
        "    model_inputs = torch.stack([xx1, xx2], dim=-1)\n",
        "    preds = model(model_inputs)\n",
        "    preds = torch.sigmoid(preds)\n",
        "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
        "    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
        "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
        "    plt.grid(False)\n",
        "    return fig\n",
        "\n",
        "_ = visualize_classification(model, dataset.data, dataset.label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD-sNap4i5MA"
      },
      "source": [
        "### Optional Dynamic Computation Graph and Backpropagation\n",
        "\n",
        "We can automatically get **gradients/derivatives** of functions that we define.\n",
        "\n",
        "Given an input $\\mathbf{x}$, we define our function by **manipulating** that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input.\n",
        "\n",
        "The first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a8sJiUHi5MA"
      },
      "outputs": [],
      "source": [
        "x = torch.ones((3,))\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uehuuykzi5ME"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_(True)\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHcYBeni5ME"
      },
      "source": [
        "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
        "\n",
        "$$y = \\frac{1}{\\ell(x)}\\sum_i \\left[(x_i + 2)^2 + 3\\right],$$\n",
        "\n",
        "where we use $\\ell(x)$ to denote the number of elements in $x$. In other words, we are taking a mean over the operation within the sum. You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXsVYiEPi5ME"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MOxtzV0i5ME"
      },
      "source": [
        "Now let's build the computation graph step by step. Below is a visualization of the graph:\n",
        "$x$ and $2$ are the base-value inputs. $a$ performs the addition $x+2$. $b$ squares $a$, and so on. The graph is a way to represent this formula: nodes are operations and edges represent information flow dependencies.\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/pytorch_computation_graph.svg?raw=1\" width=\"200px\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJOZwqz9i5ME"
      },
      "outputs": [],
      "source": [
        "# Build the computational graph for our above function.\n",
        "# Ex: m = n * 2 creates a node m that doubles the output value of node n.\n",
        "#TODO\n",
        "a =\n",
        "b =\n",
        "c =\n",
        "y =\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzL1_SQoi5ME"
      },
      "source": [
        "We calculate $a$ based on the inputs $x$ and the constant $2$, $b$ is $a$ squared, and so on. We can perform backpropagation on the computation graph by calling the function `backward()` on the last output, which effectively calculates the gradients for each tensor that has the property `requires_grad=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezt8F0z8i5ME"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a6OORMi5ME"
      },
      "source": [
        "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f78Wr-U0i5ME"
      },
      "outputs": [],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Activation Functions"
      ],
      "metadata": {
        "id": "xKdZHI3oSEwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing and visualizing the activation functions, we are aiming to gain insights into their effect.\n",
        "We do this by using a simple neural network trained on [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) and examine various aspects of the model, including the performance and gradient flow."
      ],
      "metadata": {
        "id": "4B37RsfKSX1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qQKnOKKcSGXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "kSsL-WcWSZwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, activation_func=nn.ReLU()):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(784, 128)\n",
        "        self.activation = activation_func\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.output = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.activation(self.linear1(x))\n",
        "        x = self.activation(self.linear2(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HRgl6J1nSb-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the network on the activation functions, and plot the loss."
      ],
      "metadata": {
        "id": "y_j4k4bMSe7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(activation_func):\n",
        "    # TODO: initalize the model, set the criterion and optimizer\n",
        "\n",
        "    epochs = 5\n",
        "    loss_history = []\n",
        "\n",
        "    for e in tqdm(range(epochs)):\n",
        "        running_loss = 0\n",
        "        for images, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        loss_history.append(running_loss/len(trainloader))\n",
        "\n",
        "    plt.plot(loss_history, label=activation_func.__class__.__name__)\n",
        "\n",
        "# Train and visualize for different activation functions\n",
        "train_network(nn.Sigmoid())\n",
        "train_network(nn.ReLU())\n",
        "train_network(nn.Tanh())\n",
        "\n",
        "plt.title('Training Loss with Different Activation Functions')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kln9eikLSdea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Practice with Gradients"
      ],
      "metadata": {
        "id": "3zslpiyR4DlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that a gradient is an operator and retains the dimension of the vector-valued function it is given. Given the function $f(x)=x^2+y^2+3$, compute the gradient of $f$ at the point (1, 2).\n",
        "\n",
        "Answer: (2, 4)"
      ],
      "metadata": {
        "id": "akBCU9Vk-krl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Exercise:"
      ],
      "metadata": {
        "id": "A1cEQnYsAQ-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "def f(x):\n",
        "    return\n",
        "\n",
        "def gradient_f(x):\n",
        "    return\n",
        "\n",
        "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
        "    x =\n",
        "    for i in range(num_iterations):\n",
        "      ...\n",
        "    return x\n",
        "\n",
        "# Example usage\n",
        "minimum = gradient_descent(starting_point=0, learning_rate=0.1, num_iterations=100)\n",
        "print(\"Minimum of f(x) found at x =\", minimum)\n"
      ],
      "metadata": {
        "id": "-mtekRrIATiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c72bca5-6023-4ff0-939a-af20a24ef018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum of f(x) found at x = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7itOL3fC4Pjj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}